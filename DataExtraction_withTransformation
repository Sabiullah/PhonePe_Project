!pip install GitPython

import csv
import subprocess
import pandas as pd
import requests
import git
import pandas as pd
import os

# Clone the Github repository
os.system("git clone https://github.com/phonepe/pulse.git")

!pip install requests

import os
import json
import pandas as pd
import git

# Define the local directory where you want to clone the GitHub repository
local_repo_dir = r"/path/to/local/directory"  # Replace with your desired local directory

# GitHub repository URL
github_repo_url = "https://github.com/phonepe/pulse.git"

# Clone the GitHub repository to your local directory
if not os.path.exists(local_repo_dir):
    os.makedirs(local_repo_dir)
    repo = git.Repo.clone_from(github_repo_url, local_repo_dir)
else:
    repo = git.Repo(local_repo_dir)

# Now that the repository is cloned locally, you can work with the data
root_dir = os.path.join(local_repo_dir, "data")

# Initialize an empty list to hold dictionaries of data for each JSON file
data_list = []

# Loop over all the state folders
for state_dir in os.listdir(os.path.join(root_dir, 'aggregated/transaction/country/india/state')):  
    state_path = os.path.join(root_dir, 'aggregated/transaction/country/india/state', state_dir)
    if os.path.isdir(state_path):

        # Loop over all the year folders
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)
            if os.path.isdir(year_path):

                # Loop over all the JSON files (one for each quarter)
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        with open(os.path.join(year_path, json_file)) as f:
                            data = json.load(f)

                            # Extract the data we're interested in
                            for transaction_data in data['data']['transactionData']:
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'Quarters': int(json_file.split('.')[0]),
                                    'Transaction_Type': transaction_data['name'],
                                    'Transaction_Count': transaction_data['paymentInstruments'][0]['count'],
                                    'Transaction_Amount': transaction_data['paymentInstruments'][0]['amount']
                                }
                                data_list.append(row_dict)

# Convert list of dictionaries to a DataFrame
df1 = pd.DataFrame(data_list)
df1.head()

import os
import json
import pandas as pd

# Define the root directory where the state folders are located
root_dir = r'C:\Users\safyc\OneDrive\Documents\GitHub\pulse\data\aggregated\transaction\country\india\state'

data_list = []

# Iterate through the state directories
for state_dir in os.listdir(root_dir):
    state_path = os.path.join(root_dir, state_dir)

    if os.path.isdir(state_path):
        # Iterate through the year-wise directories within the state directory
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)

            if os.path.isdir(year_path):
                # Loop through the JSON files in each year's directory
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        json_file_path = os.path.join(year_path, json_file)
                        with open(json_file_path, 'r') as f:
                            json_data = json.load(f)

                            # Extract the data we're interested in
                            for transaction_data in json_data['data']['transactionData']:
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'Quarters': int(json_file.split('.')[0]),
                                    'Transaction_Type': transaction_data['name'],
                                    'Transaction_Count': transaction_data['paymentInstruments'][0]['count'],
                                    'Transaction_Amount': transaction_data['paymentInstruments'][0]['amount']
                                }
                                data_list.append(row_dict)

# Create a DataFrame from the collected data
df2 = pd.DataFrame(data_list)
df2.head()

import os
import json
import pandas as pd
import git

# Define the local directory where you want to clone the GitHub repository
local_repo_dir = r"C:\Users\safyc\OneDrive\Documents\GitHub\pulse"  # Local repository directory

# GitHub repository URL
github_repo_url = "https://github.com/PhonePe/pulse.git"  # GitHub repository URL

# Clone the GitHub repository to your local directory
if not os.path.exists(local_repo_dir):
    os.makedirs(local_repo_dir)
    repo = git.Repo.clone_from(github_repo_url, local_repo_dir)
else:
    repo = git.Repo(local_repo_dir)

# Now that the repository is cloned locally, you can work with the data
root_dir = os.path.join(local_repo_dir, "data", "map", "transaction", "hover", "country", "india", "state")

# Initialize an empty list to hold dictionaries of data for each JSON file
data_list = []

# Loop over all the state folders
for state_dir in os.listdir(root_dir):
    state_path = os.path.join(root_dir, state_dir)
    if os.path.isdir(state_path):

        # Loop over all the year folders within each state folder
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)
            if os.path.isdir(year_path):

                # Loop over all the JSON files in each year's directory
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        json_file_path = os.path.join(year_path, json_file)
                        with open(json_file_path, 'r') as f:
                            json_data = json.load(f)

                            # Access the 'hoverDataList' key to get the data
                            hover_data_list = json_data['data']['hoverDataList']

                            # Extract the data we're interested in from 'hoverDataList'
                            for hover_data in hover_data_list:
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'District': hover_data['name'],
                                    'Transaction_Type': 'TOTAL',
                                    'Transaction_Count': hover_data['metric'][0]['count'],
                                    'Transaction_Amount': hover_data['metric'][0]['amount']
                                }
                                data_list.append(row_dict)

# Convert the list of dictionaries to a DataFrame
df3 = pd.DataFrame(data_list)
df3

import os
import json
import pandas as pd
import git

# Define the local directory where you want to clone the GitHub repository
local_repo_dir = r"C:\Users\safyc\OneDrive\Documents\GitHub\pulse"  # Local repository directory

# GitHub repository URL
github_repo_url = "https://github.com/PhonePe/pulse.git"  # GitHub repository URL

# Clone the GitHub repository to your local directory
if not os.path.exists(local_repo_dir):
    os.makedirs(local_repo_dir)
    repo = git.Repo.clone_from(github_repo_url, local_repo_dir)
else:
    repo = git.Repo(local_repo_dir)

# Now that the repository is cloned locally, you can work with the data
root_dir = os.path.join(local_repo_dir, "data", "map", "user", "hover", "country", "india", "state")

# Initialize an empty list to hold dictionaries of data for each JSON file
data_list = []

# Loop over all the state folders
for state_dir in os.listdir(root_dir):
    state_path = os.path.join(root_dir, state_dir)
    if os.path.isdir(state_path):

        # Loop over all the year folders within each state folder
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)
            if os.path.isdir(year_path):

                # Loop over all the JSON files in each year's directory
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        json_file_path = os.path.join(year_path, json_file)
                        with open(json_file_path, 'r') as f:
                            json_data = json.load(f)

                            # Extract the data we're interested in from the JSON structure
                            for district, district_data in json_data['data']['hoverData'].items():
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'District': district,
                                    'RegisteredUsers': district_data['registeredUsers'],
                                    'AppOpens': district_data['appOpens']
                                }
                                data_list.append(row_dict)

# Convert the list of dictionaries to a DataFrame
df4 = pd.DataFrame(data_list)
df4.head()

import os
import json
import pandas as pd
import git

# Define the local directory where you want to clone the GitHub repository
local_repo_dir = r"C:\Users\safyc\OneDrive\Documents\GitHub\pulse"  # Local repository directory

# GitHub repository URL
github_repo_url = "https://github.com/PhonePe/pulse.git"  # GitHub repository URL

# Clone the GitHub repository to your local directory
if not os.path.exists(local_repo_dir):
    os.makedirs(local_repo_dir)
    repo = git.Repo.clone_from(github_repo_url, local_repo_dir)
else:
    repo = git.Repo(local_repo_dir)

# Now that the repository is cloned locally, you can work with the data
root_dir = os.path.join(local_repo_dir, "data", "top", "transaction", "country", "india", "state")

# Initialize an empty list to hold dictionaries of data for each JSON file
data_list = []

# Loop over all the state folders
for state_dir in os.listdir(root_dir):
    state_path = os.path.join(root_dir, state_dir)
    if os.path.isdir(state_path):

        # Loop over all the year folders within each state folder
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)
            if os.path.isdir(year_path):

                # Loop over all the JSON files in each year's directory
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        json_file_path = os.path.join(year_path, json_file)
                        with open(json_file_path, 'r') as f:
                            json_data = json.load(f)

                            # Extract the data we're interested in from the JSON structure
                            for district_data in json_data['data']['districts']:
                                district = district_data['entityName']
                                metrics = district_data['metric']
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'District': district,
                                    'Transaction_Type': metrics['type'],
                                    'Transaction_Count': metrics['count'],
                                    'Transaction_Amount': metrics['amount']
                                }
                                data_list.append(row_dict)

# Convert the list of dictionaries to a DataFrame
df5 = pd.DataFrame(data_list)
df5.head()

import os
import json
import pandas as pd
import git

# Define the local directory where you want to clone the GitHub repository
local_repo_dir = r"C:\Users\safyc\OneDrive\Documents\GitHub\pulse"  # Local repository directory

# GitHub repository URL
github_repo_url = "https://github.com/PhonePe/pulse.git"  # GitHub repository URL

# Clone the GitHub repository to your local directory
if not os.path.exists(local_repo_dir):
    os.makedirs(local_repo_dir)
    repo = git.Repo.clone_from(github_repo_url, local_repo_dir)
else:
    repo = git.Repo(local_repo_dir)

# Define the root directory for the data you want to extract
root_dir = os.path.join(local_repo_dir, "data", "top", "user", "country", "india", "state")

# Initialize an empty list to hold dictionaries of data for each JSON file
data_list = []

# Loop over all the state folders
for state_dir in os.listdir(root_dir):
    state_path = os.path.join(root_dir, state_dir)
    if os.path.isdir(state_path):

        # Loop over all the year folders within each state folder
        for year_dir in os.listdir(state_path):
            year_path = os.path.join(state_path, year_dir)
            if os.path.isdir(year_path):

                # Loop over all the JSON files in each year's directory
                for json_file in os.listdir(year_path):
                    if json_file.endswith('.json'):
                        json_file_path = os.path.join(year_path, json_file)
                        with open(json_file_path, 'r') as f:
                            json_data = json.load(f)

                            # Extract the data we're interested in from the JSON structure
                            for district_data in json_data['data']['districts']:
                                district = district_data['name']
                                registered_users = district_data['registeredUsers']
                                row_dict = {
                                    'States': state_dir,
                                    'Transaction_Year': year_dir,
                                    'District': district,
                                    'RegisteredUsers': registered_users
                                }
                                data_list.append(row_dict)

# Convert the list of dictionaries to a DataFrame
df6 = pd.DataFrame(data_list)
df6.head()

# Drop any duplicates
d1 = df1.drop_duplicates()
d2 = df2.drop_duplicates()
d3 = df3.drop_duplicates()
d4 = df4.drop_duplicates()
d5 = df5.drop_duplicates()

#converting all dataframes in to csv
d1.to_csv('agg_trans.csv', index=False)
df2.to_csv('agg_user.csv', index=False)
d3.to_csv('map_tran.csv', index=False)
d4.to_csv('map_user.csv', index=False)
d5.to_csv('top_tran.csv', index=False)
d6.to_csv('top_user.csv', index=False)

Agg_trans = pd.read_csv(r'agg_trans.csv')
Agg_users = pd.read_csv(r'agg_user.csv')
Map_trans = pd.read_csv(r'map_tran.csv')
Map_user = pd.read_csv(r'map_user.csv')
Top_trans = pd.read_csv(r'top_tran.csv')
Top_user = pd.read_csv(r'top_user.csv')

import mysql.connector

import mysql.connector

mydb = mysql.connector.connect(
  host="localhost",
  user="root",
  password="",
#   database="joins"

)

print(mydb)
mycursor = mydb.cursor(buffered=True)

pip install mysql-connector-python

import mysql.connector
import pandas as pd
from sqlalchemy import create_engine

# Define your MySQL connection details
db_config = {
    "host": "localhost",
    "user": "root",
    "password": "",  # Enter your MySQL password here
    "database": "phonepe_pulse"  # Your database name
}

# List of CSV files and table names
csv_files = ['agg_trans.csv', 'agg_user.csv', 'map_tran.csv', 'map_user.csv', 'top_tran.csv', 'top_user.csv']
table_names = ['agg_trans', 'agg_user', 'map_tran', 'map_user', 'top_tran', 'top_user']

# Establish a connection to the MySQL database using SQLAlchemy
engine = create_engine(f"mysql+mysqlconnector://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}")

# Loop through the files and table names
for csv_file, table_name in zip(csv_files, table_names):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)

    # Insert the data from the DataFrame into the MySQL table
    df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)

# Close the SQLAlchemy engine
engine.dispose()

mycursor.execute("SHOW TABLES FROM phonepe_pulse")
for x in mycursor:
    print(x)
